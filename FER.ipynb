{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FER.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/felayani/FER/blob/master/FER.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dThztQGMX-ye",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8iTQqcwKbMSz",
        "colab_type": "code",
        "outputId": "efd4f00a-c9c8-45e6-aea3-85d03f47dca6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXg5DjD_bSQg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset_path = os.path.join(\"/content/gdrive/My Drive/data\", \"FER/fer2013.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6JJ0dvidqD6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "image_size=(48,48)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "war7X9Xsifrj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = pd.read_csv(dataset_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PbgUrHs3dq4y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_fer2013():\n",
        "    data = pd.read_csv(dataset_path)\n",
        "    pixels = data['pixels'].tolist()\n",
        "    width, height = 48, 48\n",
        "    faces = []\n",
        "    for pixel_sequence in pixels:\n",
        "        face = [int(pixel) for pixel in pixel_sequence.split(' ')]\n",
        "        face = np.asarray(face).reshape(width, height)\n",
        "        face = cv2.resize(face.astype('uint8'),image_size)\n",
        "        faces.append(face.astype('float32'))\n",
        "    faces = np.asarray(faces)\n",
        "    faces = np.expand_dims(faces, -1)\n",
        "    emotions = pd.get_dummies(data['emotion']).as_matrix()\n",
        "    return faces, emotions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IdfSia10dtgw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_input(x, v2=True):\n",
        "    x = x.astype('float32')\n",
        "    x = x / 255.0\n",
        "    if v2:\n",
        "        x = x - 0.5\n",
        "        x = x * 2.0\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNw2dCsRdv7M",
        "colab_type": "code",
        "outputId": "7aa50505-4acc-486c-b77f-37a04e787e38",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "faces, emotions = load_fer2013()\n",
        "faces = preprocess_input(faces)\n",
        "xtrain, xtest,ytrain,ytest = train_test_split(faces, emotions,test_size=0.2,shuffle=True)\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
            "  del sys.path[0]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDjmFPy4dySf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.image import img_to_array\n",
        "import imutils\n",
        "import cv2\n",
        "from keras.models import load_model\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSebzq01pNaS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "outputId": "7d763646-ad7e-4de0-835e-4f8809d84e34"
      },
      "source": [
        "# parameters for loading data and images\n",
        "detection_model_path = os.path.join(\"/content/gdrive/My Drive/data\", \"haarcascade_files/haarcascade_frontalface_default.xml\")\n",
        "emotion_model_path = os.path.join(\"/content/gdrive/My Drive/data\", \"models/_mini_XCEPTION.102-0.66.hdf5\")\n",
        "\n",
        "# hyper-parameters for bounding boxes shape\n",
        "# loading models\n",
        "face_detection = cv2.CascadeClassifier(detection_model_path)\n",
        "emotion_classifier = load_model(emotion_model_path, compile=False)\n",
        "EMOTIONS = [\"angry\" ,\"disgust\",\"scared\", \"happy\", \"sad\", \"surprised\",\n",
        " \"neutral\"]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6dRA4ikpRBv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# starting video streaming\n",
        "cv2.namedWindow('your_face')\n",
        "camera = cv2.VideoCapture(0)\n",
        "while True:\n",
        "    frame = camera.read()[1]\n",
        "    #reading the frame\n",
        "    frame = imutils.resize(frame,width=300)\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "    faces = face_detection.detectMultiScale(gray,scaleFactor=1.1,minNeighbors=5,minSize=(30,30),flags=cv2.CASCADE_SCALE_IMAGE)\n",
        "    \n",
        "    canvas = np.zeros((250, 300, 3), dtype=\"uint8\")\n",
        "    frameClone = frame.copy()\n",
        "    if len(faces) > 0:\n",
        "        faces = sorted(faces, reverse=True,\n",
        "        key=lambda x: (x[2] - x[0]) * (x[3] - x[1]))[0]\n",
        "        (fX, fY, fW, fH) = faces\n",
        "                    # Extract the ROI of the face from the grayscale image, resize it to a fixed 28x28 pixels, and then prepare\n",
        "            # the ROI for classification via the CNN\n",
        "        roi = gray[fY:fY + fH, fX:fX + fW]\n",
        "        roi = cv2.resize(roi, (64, 64))\n",
        "        roi = roi.astype(\"float\") / 255.0\n",
        "        roi = img_to_array(roi)\n",
        "        roi = np.expand_dims(roi, axis=0)\n",
        "        \n",
        "        \n",
        "        preds = emotion_classifier.predict(roi)[0]\n",
        "        emotion_probability = np.max(preds)\n",
        "        label = EMOTIONS[preds.argmax()]\n",
        "    else: continue\n",
        "\n",
        " \n",
        "    for (i, (emotion, prob)) in enumerate(zip(EMOTIONS, preds)):\n",
        "                # construct the label text\n",
        "                text = \"{}: {:.2f}%\".format(emotion, prob * 100)\n",
        "\n",
        "                # draw the label + probability bar on the canvas\n",
        "               # emoji_face = feelings_faces[np.argmax(preds)]\n",
        "\n",
        "                \n",
        "                w = int(prob * 300)\n",
        "                cv2.rectangle(canvas, (7, (i * 35) + 5),\n",
        "                (w, (i * 35) + 35), (0, 0, 255), -1)\n",
        "                cv2.putText(canvas, text, (10, (i * 35) + 23),\n",
        "                cv2.FONT_HERSHEY_SIMPLEX, 0.45,\n",
        "                (255, 255, 255), 2)\n",
        "                cv2.putText(frameClone, label, (fX, fY - 10),\n",
        "                cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 0, 255), 2)\n",
        "                cv2.rectangle(frameClone, (fX, fY), (fX + fW, fY + fH),\n",
        "                              (0, 0, 255), 2)\n",
        "#    for c in range(0, 3):\n",
        "#        frame[200:320, 10:130, c] = emoji_face[:, :, c] * \\\n",
        "#        (emoji_face[:, :, 3] / 255.0) + frame[200:320,\n",
        "#        10:130, c] * (1.0 - emoji_face[:, :, 3] / 255.0)\n",
        "\n",
        "\n",
        "    cv2.imshow('your_face', frameClone)\n",
        "    cv2.imshow(\"Probabilities\", canvas)\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "camera.release()\n",
        "cv2.destroyAllWindows()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_UAYf1yrGds",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8dca55b6-ce07-4282-9dd9-591e39b5929e"
      },
      "source": [
        "from keras.layers import Activation, Convolution2D, Dropout, Conv2D\n",
        "from keras.layers import AveragePooling2D, BatchNormalization\n",
        "from keras.layers import GlobalAveragePooling2D\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Flatten\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import SeparableConv2D\n",
        "from keras import layers\n",
        "from keras.regularizers import l2\n",
        "\n",
        "def simple_CNN(input_shape, num_classes):\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Convolution2D(filters=16, kernel_size=(7, 7), padding='same',\n",
        "                            name='image_array', input_shape=input_shape))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Convolution2D(filters=16, kernel_size=(7, 7), padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(AveragePooling2D(pool_size=(2, 2), padding='same'))\n",
        "    model.add(Dropout(.5))\n",
        "\n",
        "    model.add(Convolution2D(filters=32, kernel_size=(5, 5), padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Convolution2D(filters=32, kernel_size=(5, 5), padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(AveragePooling2D(pool_size=(2, 2), padding='same'))\n",
        "    model.add(Dropout(.5))\n",
        "\n",
        "    model.add(Convolution2D(filters=64, kernel_size=(3, 3), padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Convolution2D(filters=64, kernel_size=(3, 3), padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(AveragePooling2D(pool_size=(2, 2), padding='same'))\n",
        "    model.add(Dropout(.5))\n",
        "\n",
        "    model.add(Convolution2D(filters=128, kernel_size=(3, 3), padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Convolution2D(filters=128, kernel_size=(3, 3), padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(AveragePooling2D(pool_size=(2, 2), padding='same'))\n",
        "    model.add(Dropout(.5))\n",
        "\n",
        "    model.add(Convolution2D(filters=256, kernel_size=(3, 3), padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Convolution2D(filters=num_classes, kernel_size=(3, 3), padding='same'))\n",
        "    model.add(GlobalAveragePooling2D())\n",
        "    model.add(Activation('softmax',name='predictions'))\n",
        "    return model\n",
        "\n",
        "def simpler_CNN(input_shape, num_classes):\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Convolution2D(filters=16, kernel_size=(5, 5), padding='same',\n",
        "                            name='image_array', input_shape=input_shape))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Convolution2D(filters=16, kernel_size=(5, 5),\n",
        "                            strides=(2, 2), padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout(.25))\n",
        "\n",
        "    model.add(Convolution2D(filters=32, kernel_size=(5, 5), padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Convolution2D(filters=32, kernel_size=(5, 5),\n",
        "                            strides=(2, 2), padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout(.25))\n",
        "\n",
        "    model.add(Convolution2D(filters=64, kernel_size=(3, 3), padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Convolution2D(filters=64, kernel_size=(3, 3),\n",
        "                            strides=(2, 2), padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout(.25))\n",
        "\n",
        "    model.add(Convolution2D(filters=64, kernel_size=(1, 1), padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Convolution2D(filters=128, kernel_size=(3, 3),\n",
        "                            strides=(2, 2), padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout(.25))\n",
        "\n",
        "    model.add(Convolution2D(filters=256, kernel_size=(1, 1), padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Convolution2D(filters=128, kernel_size=(3, 3),\n",
        "                            strides=(2, 2), padding='same'))\n",
        "\n",
        "    model.add(Convolution2D(filters=256, kernel_size=(1, 1), padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Convolution2D(filters=num_classes, kernel_size=(3, 3),\n",
        "                            strides=(2, 2), padding='same'))\n",
        "\n",
        "    model.add(Flatten())\n",
        "    #model.add(GlobalAveragePooling2D())\n",
        "    model.add(Activation('softmax',name='predictions'))\n",
        "    return model\n",
        "\n",
        "def tiny_XCEPTION(input_shape, num_classes, l2_regularization=0.01):\n",
        "    regularization = l2(l2_regularization)\n",
        "\n",
        "    # base\n",
        "    img_input = Input(input_shape)\n",
        "    x = Conv2D(5, (3, 3), strides=(1, 1), kernel_regularizer=regularization,\n",
        "                                            use_bias=False)(img_input)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Conv2D(5, (3, 3), strides=(1, 1), kernel_regularizer=regularization,\n",
        "                                            use_bias=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    # module 1\n",
        "    residual = Conv2D(8, (1, 1), strides=(2, 2),\n",
        "                      padding='same', use_bias=False)(x)\n",
        "    residual = BatchNormalization()(residual)\n",
        "\n",
        "    x = SeparableConv2D(8, (3, 3), padding='same',\n",
        "                        kernel_regularizer=regularization,\n",
        "                        use_bias=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = SeparableConv2D(8, (3, 3), padding='same',\n",
        "                        kernel_regularizer=regularization,\n",
        "                        use_bias=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    x = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
        "    x = layers.add([x, residual])\n",
        "\n",
        "    # module 2\n",
        "    residual = Conv2D(16, (1, 1), strides=(2, 2),\n",
        "                      padding='same', use_bias=False)(x)\n",
        "    residual = BatchNormalization()(residual)\n",
        "\n",
        "    x = SeparableConv2D(16, (3, 3), padding='same',\n",
        "                        kernel_regularizer=regularization,\n",
        "                        use_bias=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = SeparableConv2D(16, (3, 3), padding='same',\n",
        "                        kernel_regularizer=regularization,\n",
        "                        use_bias=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    x = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
        "    x = layers.add([x, residual])\n",
        "\n",
        "    # module 3\n",
        "    residual = Conv2D(32, (1, 1), strides=(2, 2),\n",
        "                      padding='same', use_bias=False)(x)\n",
        "    residual = BatchNormalization()(residual)\n",
        "\n",
        "    x = SeparableConv2D(32, (3, 3), padding='same',\n",
        "                        kernel_regularizer=regularization,\n",
        "                        use_bias=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = SeparableConv2D(32, (3, 3), padding='same',\n",
        "                        kernel_regularizer=regularization,\n",
        "                        use_bias=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    x = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
        "    x = layers.add([x, residual])\n",
        "\n",
        "    # module 4\n",
        "    residual = Conv2D(64, (1, 1), strides=(2, 2),\n",
        "                      padding='same', use_bias=False)(x)\n",
        "    residual = BatchNormalization()(residual)\n",
        "\n",
        "    x = SeparableConv2D(64, (3, 3), padding='same',\n",
        "                        kernel_regularizer=regularization,\n",
        "                        use_bias=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = SeparableConv2D(64, (3, 3), padding='same',\n",
        "                        kernel_regularizer=regularization,\n",
        "                        use_bias=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    x = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
        "    x = layers.add([x, residual])\n",
        "\n",
        "    x = Conv2D(num_classes, (3, 3),\n",
        "            #kernel_regularizer=regularization,\n",
        "            padding='same')(x)\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    output = Activation('softmax',name='predictions')(x)\n",
        "\n",
        "    model = Model(img_input, output)\n",
        "    return model\n",
        "\n",
        "\n",
        "def mini_XCEPTION(input_shape, num_classes, l2_regularization=0.01):\n",
        "    regularization = l2(l2_regularization)\n",
        "\n",
        "    # base\n",
        "    img_input = Input(input_shape)\n",
        "    x = Conv2D(8, (3, 3), strides=(1, 1), kernel_regularizer=regularization,\n",
        "                                            use_bias=False)(img_input)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Conv2D(8, (3, 3), strides=(1, 1), kernel_regularizer=regularization,\n",
        "                                            use_bias=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    # module 1\n",
        "    residual = Conv2D(16, (1, 1), strides=(2, 2),\n",
        "                      padding='same', use_bias=False)(x)\n",
        "    residual = BatchNormalization()(residual)\n",
        "\n",
        "    x = SeparableConv2D(16, (3, 3), padding='same',\n",
        "                        kernel_regularizer=regularization,\n",
        "                        use_bias=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = SeparableConv2D(16, (3, 3), padding='same',\n",
        "                        kernel_regularizer=regularization,\n",
        "                        use_bias=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    x = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
        "    x = layers.add([x, residual])\n",
        "\n",
        "    # module 2\n",
        "    residual = Conv2D(32, (1, 1), strides=(2, 2),\n",
        "                      padding='same', use_bias=False)(x)\n",
        "    residual = BatchNormalization()(residual)\n",
        "\n",
        "    x = SeparableConv2D(32, (3, 3), padding='same',\n",
        "                        kernel_regularizer=regularization,\n",
        "                        use_bias=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = SeparableConv2D(32, (3, 3), padding='same',\n",
        "                        kernel_regularizer=regularization,\n",
        "                        use_bias=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    x = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
        "    x = layers.add([x, residual])\n",
        "\n",
        "    # module 3\n",
        "    residual = Conv2D(64, (1, 1), strides=(2, 2),\n",
        "                      padding='same', use_bias=False)(x)\n",
        "    residual = BatchNormalization()(residual)\n",
        "\n",
        "    x = SeparableConv2D(64, (3, 3), padding='same',\n",
        "                        kernel_regularizer=regularization,\n",
        "                        use_bias=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = SeparableConv2D(64, (3, 3), padding='same',\n",
        "                        kernel_regularizer=regularization,\n",
        "                        use_bias=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    x = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
        "    x = layers.add([x, residual])\n",
        "\n",
        "    # module 4\n",
        "    residual = Conv2D(128, (1, 1), strides=(2, 2),\n",
        "                      padding='same', use_bias=False)(x)\n",
        "    residual = BatchNormalization()(residual)\n",
        "\n",
        "    x = SeparableConv2D(128, (3, 3), padding='same',\n",
        "                        kernel_regularizer=regularization,\n",
        "                        use_bias=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = SeparableConv2D(128, (3, 3), padding='same',\n",
        "                        kernel_regularizer=regularization,\n",
        "                        use_bias=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    x = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
        "    x = layers.add([x, residual])\n",
        "\n",
        "    x = Conv2D(num_classes, (3, 3),\n",
        "            #kernel_regularizer=regularization,\n",
        "            padding='same')(x)\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    output = Activation('softmax',name='predictions')(x)\n",
        "\n",
        "    model = Model(img_input, output)\n",
        "    return model\n",
        "\n",
        "def big_XCEPTION(input_shape, num_classes):\n",
        "    img_input = Input(input_shape)\n",
        "    x = Conv2D(32, (3, 3), strides=(2, 2), use_bias=False)(img_input)\n",
        "    x = BatchNormalization(name='block1_conv1_bn')(x)\n",
        "    x = Activation('relu', name='block1_conv1_act')(x)\n",
        "    x = Conv2D(64, (3, 3), use_bias=False)(x)\n",
        "    x = BatchNormalization(name='block1_conv2_bn')(x)\n",
        "    x = Activation('relu', name='block1_conv2_act')(x)\n",
        "\n",
        "    residual = Conv2D(128, (1, 1), strides=(2, 2),\n",
        "                      padding='same', use_bias=False)(x)\n",
        "    residual = BatchNormalization()(residual)\n",
        "\n",
        "    x = SeparableConv2D(128, (3, 3), padding='same', use_bias=False)(x)\n",
        "    x = BatchNormalization(name='block2_sepconv1_bn')(x)\n",
        "    x = Activation('relu', name='block2_sepconv2_act')(x)\n",
        "    x = SeparableConv2D(128, (3, 3), padding='same', use_bias=False)(x)\n",
        "    x = BatchNormalization(name='block2_sepconv2_bn')(x)\n",
        "\n",
        "    x = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
        "    x = layers.add([x, residual])\n",
        "\n",
        "    residual = Conv2D(256, (1, 1), strides=(2, 2),\n",
        "                      padding='same', use_bias=False)(x)\n",
        "    residual = BatchNormalization()(residual)\n",
        "\n",
        "    x = Activation('relu', name='block3_sepconv1_act')(x)\n",
        "    x = SeparableConv2D(256, (3, 3), padding='same', use_bias=False)(x)\n",
        "    x = BatchNormalization(name='block3_sepconv1_bn')(x)\n",
        "    x = Activation('relu', name='block3_sepconv2_act')(x)\n",
        "    x = SeparableConv2D(256, (3, 3), padding='same', use_bias=False)(x)\n",
        "    x = BatchNormalization(name='block3_sepconv2_bn')(x)\n",
        "\n",
        "    x = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
        "    x = layers.add([x, residual])\n",
        "    x = Conv2D(num_classes, (3, 3),\n",
        "            #kernel_regularizer=regularization,\n",
        "            padding='same')(x)\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    output = Activation('softmax',name='predictions')(x)\n",
        "\n",
        "    model = Model(img_input, output)\n",
        "    return model\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    input_shape = (64, 64, 1)\n",
        "    num_classes = 7\n",
        "    #model = tiny_XCEPTION(input_shape, num_classes)\n",
        "    #model.summary()\n",
        "    #model = mini_XCEPTION(input_shape, num_classes)\n",
        "    #model.summary()\n",
        "    #model = big_XCEPTION(input_shape, num_classes)\n",
        "    #model.summary()\n",
        "    model = simple_CNN((48, 48, 1), num_classes)\n",
        "    model.summary()\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4271: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "image_array (Conv2D)         (None, 48, 48, 16)        800       \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 48, 48, 16)        64        \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 48, 48, 16)        12560     \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 48, 48, 16)        64        \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 48, 48, 16)        0         \n",
            "_________________________________________________________________\n",
            "average_pooling2d_1 (Average (None, 24, 24, 16)        0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 24, 24, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 24, 24, 32)        12832     \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 24, 24, 32)        128       \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 24, 24, 32)        25632     \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 24, 24, 32)        128       \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 24, 24, 32)        0         \n",
            "_________________________________________________________________\n",
            "average_pooling2d_2 (Average (None, 12, 12, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 12, 12, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 12, 12, 64)        18496     \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 12, 12, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 12, 12, 64)        36928     \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 12, 12, 64)        256       \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 12, 12, 64)        0         \n",
            "_________________________________________________________________\n",
            "average_pooling2d_3 (Average (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 6, 6, 128)         73856     \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 6, 6, 128)         512       \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 6, 6, 128)         147584    \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 6, 6, 128)         512       \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 6, 6, 128)         0         \n",
            "_________________________________________________________________\n",
            "average_pooling2d_4 (Average (None, 3, 3, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 3, 3, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 3, 3, 256)         295168    \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 3, 3, 256)         1024      \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 3, 3, 7)           16135     \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_1 ( (None, 7)                 0         \n",
            "_________________________________________________________________\n",
            "predictions (Activation)     (None, 7)                 0         \n",
            "=================================================================\n",
            "Total params: 642,935\n",
            "Trainable params: 641,463\n",
            "Non-trainable params: 1,472\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxQeP9oMuMQy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import CSVLogger, ModelCheckpoint, EarlyStopping\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nlYcE_ZeugX8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# parameters\n",
        "batch_size = 32\n",
        "num_epochs = 10000\n",
        "input_shape = (48, 48, 1)\n",
        "validation_split = .2\n",
        "verbose = 1\n",
        "num_classes = 7\n",
        "patience = 50\n",
        "base_path = os.path.join(\"/content/gdrive/My Drive/data\", \"models\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6FbIxOTuwsI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "06fa27b9-3e5c-48fe-e206-378bdc3319b9"
      },
      "source": [
        "# data generator\n",
        "data_generator = ImageDataGenerator(\n",
        "                        featurewise_center=False,\n",
        "                        featurewise_std_normalization=False,\n",
        "                        rotation_range=10,\n",
        "                        width_shift_range=0.1,\n",
        "                        height_shift_range=0.1,\n",
        "                        zoom_range=.1,\n",
        "                        horizontal_flip=True)\n",
        "\n",
        "# model parameters/compilation\n",
        "model = mini_XCEPTION(input_shape, num_classes)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 48, 48, 1)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 46, 46, 8)    72          input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 46, 46, 8)    32          conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 46, 46, 8)    0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 44, 44, 8)    576         activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 44, 44, 8)    32          conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 44, 44, 8)    0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_1 (SeparableCo (None, 44, 44, 16)   200         activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 44, 44, 16)   64          separable_conv2d_1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 44, 44, 16)   0           batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_2 (SeparableCo (None, 44, 44, 16)   400         activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 44, 44, 16)   64          separable_conv2d_2[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 22, 22, 16)   128         activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 22, 22, 16)   0           batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 22, 22, 16)   64          conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 22, 22, 16)   0           max_pooling2d_1[0][0]            \n",
            "                                                                 batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_3 (SeparableCo (None, 22, 22, 32)   656         add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 22, 22, 32)   128         separable_conv2d_3[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 22, 22, 32)   0           batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_4 (SeparableCo (None, 22, 22, 32)   1312        activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_17 (BatchNo (None, 22, 22, 32)   128         separable_conv2d_4[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 11, 11, 32)   512         add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 11, 11, 32)   0           batch_normalization_17[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 11, 11, 32)   128         conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_2 (Add)                     (None, 11, 11, 32)   0           max_pooling2d_2[0][0]            \n",
            "                                                                 batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_5 (SeparableCo (None, 11, 11, 64)   2336        add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_19 (BatchNo (None, 11, 11, 64)   256         separable_conv2d_5[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 11, 11, 64)   0           batch_normalization_19[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_6 (SeparableCo (None, 11, 11, 64)   4672        activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_20 (BatchNo (None, 11, 11, 64)   256         separable_conv2d_6[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 6, 6, 64)     2048        add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2D)  (None, 6, 6, 64)     0           batch_normalization_20[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_18 (BatchNo (None, 6, 6, 64)     256         conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_3 (Add)                     (None, 6, 6, 64)     0           max_pooling2d_3[0][0]            \n",
            "                                                                 batch_normalization_18[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_7 (SeparableCo (None, 6, 6, 128)    8768        add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_22 (BatchNo (None, 6, 6, 128)    512         separable_conv2d_7[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 6, 6, 128)    0           batch_normalization_22[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_8 (SeparableCo (None, 6, 6, 128)    17536       activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_23 (BatchNo (None, 6, 6, 128)    512         separable_conv2d_8[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 3, 3, 128)    8192        add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2D)  (None, 3, 3, 128)    0           batch_normalization_23[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_21 (BatchNo (None, 3, 3, 128)    512         conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_4 (Add)                     (None, 3, 3, 128)    0           max_pooling2d_4[0][0]            \n",
            "                                                                 batch_normalization_21[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 3, 3, 7)      8071        add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_2 (Glo (None, 7)            0           conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "predictions (Activation)        (None, 7)            0           global_average_pooling2d_2[0][0] \n",
            "==================================================================================================\n",
            "Total params: 58,423\n",
            "Trainable params: 56,951\n",
            "Non-trainable params: 1,472\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21YJ65vzu0hZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bae52b06-4970-416a-a9dd-5398bf36fd4c"
      },
      "source": [
        "    # callbacks\n",
        "\n",
        "log_file_path = os.path.join(\"/content/gdrive/My Drive/data\", \"models/_emotion_training.log\")\n",
        "csv_logger = CSVLogger(log_file_path, append=False)\n",
        "early_stop = EarlyStopping('val_loss', patience=patience)\n",
        "reduce_lr = ReduceLROnPlateau('val_loss', factor=0.1,\n",
        "                                  patience=int(patience/4), verbose=1)\n",
        "trained_models_path = os.path.join(\"/content/gdrive/My Drive/data\", \"models/_mini_XCEPTION\")\n",
        "model_names = trained_models_path + '.{epoch:02d}-{val_acc:.2f}.hdf5'\n",
        "model_checkpoint = ModelCheckpoint(model_names, 'val_loss', verbose=1,\n",
        "                                                    save_best_only=True)\n",
        "callbacks = [model_checkpoint, csv_logger, early_stop, reduce_lr]\n",
        "\n",
        "# loading dataset\n",
        "faces, emotions = load_fer2013()\n",
        "faces = preprocess_input(faces)\n",
        "num_samples, num_classes = emotions.shape\n",
        "xtrain, xtest,ytrain,ytest = train_test_split(faces, emotions,test_size=0.2,shuffle=True)\n",
        "model.fit_generator(data_generator.flow(xtrain, ytrain,\n",
        "                                            batch_size),\n",
        "                        steps_per_epoch=len(xtrain) / batch_size,\n",
        "                        epochs=num_epochs, verbose=1, callbacks=callbacks,\n",
        "                        validation_data=(xtest,ytest))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
            "  del sys.path[0]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10000\n",
            "898/897 [==============================] - 39s 44ms/step - loss: 1.4783 - acc: 0.4463 - val_loss: 1.4829 - val_acc: 0.4558\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.48290, saving model to /content/gdrive/My Drive/data/models/_mini_XCEPTION.01-0.46.hdf5\n",
            "Epoch 2/10000\n",
            "898/897 [==============================] - 39s 43ms/step - loss: 1.3744 - acc: 0.4827 - val_loss: 1.3159 - val_acc: 0.5110\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.48290 to 1.31594, saving model to /content/gdrive/My Drive/data/models/_mini_XCEPTION.02-0.51.hdf5\n",
            "Epoch 3/10000\n",
            "898/897 [==============================] - 39s 44ms/step - loss: 1.3100 - acc: 0.5104 - val_loss: 1.4519 - val_acc: 0.4731\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 1.31594\n",
            "Epoch 4/10000\n",
            "898/897 [==============================] - 39s 43ms/step - loss: 1.2691 - acc: 0.5236 - val_loss: 1.2372 - val_acc: 0.5454\n",
            "\n",
            "Epoch 00004: val_loss improved from 1.31594 to 1.23723, saving model to /content/gdrive/My Drive/data/models/_mini_XCEPTION.04-0.55.hdf5\n",
            "Epoch 5/10000\n",
            "898/897 [==============================] - 39s 44ms/step - loss: 1.2415 - acc: 0.5370 - val_loss: 1.3217 - val_acc: 0.5203\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 1.23723\n",
            "Epoch 6/10000\n",
            "898/897 [==============================] - 39s 43ms/step - loss: 1.2127 - acc: 0.5468 - val_loss: 1.2622 - val_acc: 0.5398\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 1.23723\n",
            "Epoch 7/10000\n",
            "898/897 [==============================] - 39s 43ms/step - loss: 1.1882 - acc: 0.5600 - val_loss: 1.1849 - val_acc: 0.5607\n",
            "\n",
            "Epoch 00007: val_loss improved from 1.23723 to 1.18488, saving model to /content/gdrive/My Drive/data/models/_mini_XCEPTION.07-0.56.hdf5\n",
            "Epoch 8/10000\n",
            "898/897 [==============================] - 39s 43ms/step - loss: 1.1769 - acc: 0.5559 - val_loss: 1.1460 - val_acc: 0.5783\n",
            "\n",
            "Epoch 00008: val_loss improved from 1.18488 to 1.14603, saving model to /content/gdrive/My Drive/data/models/_mini_XCEPTION.08-0.58.hdf5\n",
            "Epoch 9/10000\n",
            "898/897 [==============================] - 39s 43ms/step - loss: 1.1558 - acc: 0.5644 - val_loss: 1.1920 - val_acc: 0.5634\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 1.14603\n",
            "Epoch 10/10000\n",
            "113/897 [==>...........................] - ETA: 33s - loss: 1.1497 - acc: 0.5755"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-dc5ddd0b29de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxtrain\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m                         validation_data=(xtest,ytest))\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1656\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1657\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1658\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1659\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    213\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    214\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1447\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1448\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1449\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1450\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2977\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2979\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2980\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2981\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2935\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2937\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2938\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwuuupmcvXKg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}